#Deep Learning

# Text-to-image-synthesis-using-GAN


# Dataset and feature 
In order to develop and assess models that generate both text and images, it is necessary to have a set of image-text pairs. The COCO Captions Dataset [13] is a commonly used resource for such data, as it contains more than 330,000 images and 1.5 million corresponding captions. The dataset is typically divided into three subsets: around 120K images for training, 5K for validation, and 205K for testing. However, for future research, it might be advantageous to consider the Conceptual Captions dataset [12] created by Google AI, which includes over 3 million images with associated captions in a TSV file. The images in this dataset were resized to 32 x 32 pixels due to computing constraints. A few sample images from the dataset are presented below.

![image](https://github.com/shreyanshsharmaji/Text-to-image-synthesis-using-GAN/assets/99886386/a3628f50-b0cb-4691-a905-244c0da8d2b6)


![image](https://github.com/shreyanshsharmaji/Text-to-image-synthesis-using-GAN/assets/99886386/c2ed6eb5-0866-4933-99f2-cb00fd28ad1e)
